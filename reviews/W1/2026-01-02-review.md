## Review - 오늘 한 일 & 배운 점 (1월 2일)

### 1. 회고 작성 방법에 대한 조언

- 회고를 통해서 좋은 학습 기계를 만들어 내는 것이 중요하다.
- 내 학습 기계는 아무도 모른다. 그렇기 때문에 드러내야 하고, 피드백을 받는 것이 중요하다.
- 문제를 인식했을 때, 그것을 어떻게 해결할 것인지 구체적인 목표를 제시하는 것이 중요하다.
  - 예를 들어, 코드를 작성하기 전 **5분간** 공식 문서를 읽어보기

### 2. 팀 위키 리뷰

- 어떠한 주장을 할 때, 그에 대한 근거를 명확히 제시하는 것이 중요하다.
  - `~카더라` 식의 주장만 해서는 설득력이 떨어진다.
- 데이터에서 상관 계수가 높게 나왔다고 해서, 그것이 인과 관계를 의미하지는 않는다.
  - 상관 관계와 인과 관계의 차이를 명확히 이해하는 것이 중요하다.
- 분석할 수 있는 데이터가 많더라도, 질문 자체가 명확하지 않으면 좋은 결과를 얻기 어렵다.
  - 항상 질문을 명확히 하고, 그에 맞는 데이터를 선택하는 것이 중요하다.

### 3. W1M3 ETL 프로세스 구현하기 코드 리뷰

- Extract 단계에서 추출한 데이터도 **Human Readable** 해야 한다.
  - 데이터가 어떤 형태로 저장되어 있는지, 어떤 의미를 가지는지 명확히 이해할 수 있어야 한다.
- 대용량 데이터를 다룰 때는, 메모리 사용량을 고려해야 한다. 그리고 대용량 데이터를 처리하기 위한 라이브러리가 `pandas`이다.
  - Python의 Dictionary 자료형은 메모리 사용량이 상대적으로 매우 크기 때문에, 대용량 데이터를 다룰 때는 `pandas` DataFrame을 사용하는 것이 권장된다.
  - 추가) `pandas`는 내부적으로 `numpy` 배열을 사용하여 메모리 효율성을 높이고, 벡터화 연산을 통해 대용량 데이터 처리 성능을 향상시킨다.

### 4. 팀 회고 진행

- 강의에서 다뤘던 팀 위키 리뷰를 바탕으로, 우리 팀의 회고를 진행했다.
- 데이터셋이 활용될 수 있는 상황에 대해 작성한 글에서, 우리의 주장을 뒷받침할 근거를 찾지 않았던 점이 아쉬웠다.
  - 어떠한 주장을 할 때, 사람들이 납득할 수 있는 근거를 제시하는 것이 중요하다.
- 우리가 제안했던 과거 데이터 조회 가능 ETL 프로세스에 대해, 데이터의 크기가 매우 큰 경우에 대해 추가로 토론했다.
  - 큰 데이터 속에서 특정 시점의 데이터를 조회하기 위해서는, 날짜 기준 파티셔닝 등의 기법을 활용하는 것도 좋은 방법이라는 의견을 제시했다.

## Retrospect - 회고 & 개선점

### 1. Keep

- 오늘 강의에서 ETL 파이프라인을 구현할 때 항상 대규모 데이터인 상황을 고려해야 한다는 점을 배웠는데, 이를 팀 회고 시간에 우리가 제안했던 ETL 파이프라인에 직접 적용해보았다.
  - 덕분에 우리가 제안했던 ETL 파이프라인에 대용량 데이터가 포함될 경우 특정 시점 데이터를 조회하는 데 성능 이슈가 발생할 수 있다는 점을 깨달았다.
  - 이 성능 문제를 해결하기 위해 파티셔닝 기법 등의 방법에 대해 팀원들과 함께 고민해보았다.

### 2. Problem

- 지금까지 KPT 형식의 회고를 작성하면서, Problem 항목에 문제를 작성하고 개선 방안을 않았다.
  - 하지만 회고는 내 학습 기계를 개선시키기 위해 작성하는 것이다.
  - 문제를 인식했을 때, 그것을 어떻게 개선할 것인지 구체적인 목표를 제시하는 것은 너무나 중요한 일이다.
  - Problem 항목에 문제를 작성할 때, 반드시 그것을 어떻게 개선할 것인지에 대한 구체적인 목표를 함께 작성해야겠다.

### 3. Try

- 오전 수업을 들을 때 가끔, 배가 고파서 집중력이 떨어지는 문제가 있는 것 같다. 다음주부터는 콘푸레이크와 같은 간단한 아침 식사를 하고 집에서 나와야겠다.
